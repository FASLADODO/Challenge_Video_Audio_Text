{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from math import log\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.corpus import wordnet\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Plotly imports\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pydeepl\n",
    "import codecs\n",
    "from scipy.misc import imread\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mickael/anaconda3/envs/TAV/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/home/mickael/Documents/Challenge_Video_Audio_Text/features/text/sequence_text.csv', sep='§')\n",
    "\n",
    "data = data.groupby(['Sequence'])['Text'].sum() # Découper par séquence ou réplique\n",
    "data = data.reset_index()\n",
    "\n",
    "data['Text'] = [x.lower() for x in data['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('french'))\n",
    "stop.update(['.', ',', '\"', \"'\", '?', '!', ':',\n",
    "                   ';', '(', ')', '[', ']', '{', '}','-','...', '..', '«', '»' ,\"'\", \"’\", \"``\", \"''\",\n",
    "            'le', 'la', 'les', 'un', 'une', 'des', 'or', 'ni', 'car', 'assez', 'aussitôt', 'assez', \n",
    "             'car', 'des', 'la', 'le', 'les', 'ni', 'or', 'un', 'une',\n",
    "            'a', \"C'est\", 'Vous', 'ça', \"c'est\", 'Et', 'Plus', 'Mais', 'Tout', \"qu'il\",\n",
    "            'Le', 'quoi', \"qu'il\", 'si', 'là', 'Ah', 'sais', 'rien', \"j'ai\", 'Ça', 'Ce', 'Si', 'deux', 'peu',\n",
    "            'Cette', 'moi', 'oh', 'Les', 'En', 'Oh', 'A', 'Ca', 'Un', 'Va','va', 'C', \"m'a\", 'vais', 'Vais',\n",
    "            'un', 'Un', 'La', 'aussi', 'très', 'tout', 'plus', 'alors', 'faut', 'trois', 'dire', 'faire',\n",
    "            'être', 'sans', \"C'était\", 'crois', 'ils', 'Une', 'alors', 'peux', 'faut',\"qu'on\", 'Alors', 'cette', \n",
    "            'dit', \"d'un\", 'dis', 'Il', 'On', 'on', 'tu', 'Tu', \"J'ai\", \"j'ai\", 'J', 'Quand', 'quand', 'elle',\n",
    "            'tous', 'allez', 'moi', 'voilà', 'cent', 'ou', 'pour', 'hein', 'ils', \"j'en\", \"qu'est-ce\", 'mon',\n",
    "            'ans','fois', 'avec', 'fais','avec', \"n'est\", 'nous', 'vous', 'fait', 'moi', 'ah', 'puis', 'tant',\n",
    "            'autre', \"t'es\", 'juste', 'peut', 'tu', 'où', \"t'a\", \"t'as\", \"c'est\", 'autre', 'bah', 'après',\n",
    "            'comme', \"qu'elle\", \"c'était\", 'ben', 'veut', 'je', \"s'est\", 'ca', 'quelle', \"qu'elle\", 'chez',\n",
    "            \"d'une\", \"d'un\", 'ouais', 'ouai', 'tu', 'trop', 'veut', 'prend', \"d'être\", 'parce', 'vous', 'nous',\n",
    "            'toute', 'quatre', 'se', 'ce', 'tout', 'toute', 'parce', 'huit', 'quel', 'déjà', 'dix', 'huit',\n",
    "            'cela', 'Le', 'nom', 'six', \"l'a\", \"qu'il\", 'coté', \"l'ai\", \"qu'un\", \"n'a\", 'cet', 'cette', 'pour'])\n",
    "\n",
    "data['Text'] = data['Text'].apply(\n",
    "    lambda x: ' '.join([i for i in word_tokenize(x) if i not in stop])) \n",
    "# data.iloc[5] data = [data['Text']]\n",
    "#print(data.groupby('Text').apply(' '.join).reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fenetre = [] # liste des 308 séquences sous forme de fenetre\n",
    "for i in data['Text']:\n",
    "    fenetre.append([i])\n",
    "fenetre = [sentence.split() for f in fenetre for sentence in f] # une fenetre : une séquence\n",
    "#[f.split() for f in fenetre[0]]\n",
    "len(fenetre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fenetre = fenetre[0:20]\n",
    "#fenetre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = []\n",
    "for i in data['Text']:\n",
    "    liste.append(i)\n",
    "liste = \" \".join(liste).split()\n",
    "liste = [x.lower() for x in liste]\n",
    "liste = liste[0:500] \n",
    "# on réduit à 300 mots car le corpus est très long 20 000 mots temps long\n",
    "# print(len(liste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proba simple : On calcule la probabilité d'apparition d'un mot dans les fenêtres\n",
    "def proba_simple(mot, fen):\n",
    "    count = 0\n",
    "    for fenetre in fen:\n",
    "        if mot in fenetre:\n",
    "            count += 1\n",
    "    return(count/len(fen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_conjoint(mot1, mot2, fen):\n",
    "    count = 0\n",
    "    for fenetre in fen:\n",
    "        if mot1 in fenetre:\n",
    "            if mot2 in fenetre:\n",
    "                count += 1\n",
    "    return(count/len(fen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_conjointe(data):\n",
    "    freq_2mots = [[0 for i in range(len(liste))] for i in range(len(liste))]\n",
    "    for mot1 in liste:\n",
    "        for mot2 in liste:\n",
    "            if mot1 != mot2: \n",
    "                den=0\n",
    "                num=0\n",
    "                for i in fenetre:\n",
    "                    if (mot1 or mot2) in i:\n",
    "                        if (mot1 and mot2) in i:\n",
    "                            den+=1\n",
    "                        num +=1   \n",
    "                if den!= 0:\n",
    "                     freq_2mots[liste.index(mot1)][liste.index(mot2)]=den/len(liste)\n",
    "                else:\n",
    "                     freq_2mots[liste.index(mot1)][liste.index(mot2)]=0\n",
    "    return freq_2mots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_2mots = proba_conjointe(liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_2mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba(data):\n",
    "    freq_mot = [0 for i in range(len(liste))]\n",
    "    for mot1 in liste:\n",
    "        for i in fenetre:\n",
    "            if mot1 in i:\n",
    "                freq_mot[liste.index(mot1)]+=1\n",
    "        freq_mot[liste.index(mot1)] = round(freq_mot[liste.index(mot1)] / len(fenetre),2)\n",
    "    return freq_mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_mot = proba(liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(data):\n",
    "    ppmi = [[0 for i in range(len(liste))] for i in range(len(liste))]\n",
    "    for mot1 in range(len(liste)):\n",
    "        for mot2 in range(len(liste)):\n",
    "            if freq_mot[mot1]==0 or freq_mot[mot2]==0 or freq_2mots[mot1][mot2]==0 or math.log(freq_2mots[mot1][mot2] / (freq_mot[mot1]*freq_mot[mot2])) < 0:\n",
    "\n",
    "                 ppmi[mot1][mot2]=0\n",
    "            else:\n",
    "                ppmi[mot1][mot2]=math.log(freq_2mots[mot1][mot2] / (freq_mot[mot1]*freq_mot[mot2]))\n",
    "    return ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_tot = ppmi(liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calcul(mot1,mot2, freq_2mots, freq_mot, ppmi):\n",
    "    mot1=liste.index(mot1)\n",
    "    mot2=liste.index(mot2)\n",
    "    print(\"\\n Fréquence entre les 2 mots : \",freq_2mots[mot1][mot2],\"\\n Fréquence du mot 1 :  \" ,\n",
    "          freq_mot[mot1],\"\\n Fréquence du mot 2  \",freq_mot[mot2])\n",
    "    print(\" Valeur du ppmi : \", ppmi[mot1][mot2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fréquence entre les 2 mots :  0.01 \n",
      " Fréquence du mot 1 :   0.04 \n",
      " Fréquence du mot 2   0.17\n",
      " Valeur du ppmi :  0.38566248081198456\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(Calcul('bonjour', 'monsieur', freq_2mots, freq_mot, ppmi_tot)) # sur 200 / 20022 mots mais les 308 fenêtres.\n",
    "# Changer les 2 mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg \n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0.00% of fenetre\n",
      "done\n",
      "vocabulary size: 5757\n",
      "most common: [('non', 176), ('oui', 145), ('bien', 143), ('monsieur', 105), ('bon', 71), ('pourquoi', 60), ('comment', 53), ('jamais', 46), ('veux', 41), ('merci', 41)]\n"
     ]
    }
   ],
   "source": [
    "#Nous allons créer une matrice mot-mot à co-occurrence à partir du texte figurant \n",
    "#dans les titres. Nous définirons deux mots comme \"cooccurrents\" s'ils apparaissent \n",
    "#dans le même titre. En utilisant cette définition, les titres composés d’un seul \n",
    "#mot ne nous intéressent pas. Permet de les supprimer ainsi \n",
    "#qu'un ensemble commun de mots vides anglais.\n",
    "\n",
    "tok2indx = dict()\n",
    "unigram_counts = Counter()\n",
    "for ii, fenetre in enumerate(fenetre):\n",
    "    if ii % 200000 == 0:\n",
    "        print(f'finished {ii/len(fenetre):.2%} of fenetre')\n",
    "    for token in fenetre:\n",
    "        unigram_counts[token] += 1\n",
    "        if token not in tok2indx:\n",
    "            tok2indx[token] = len(tok2indx)\n",
    "indx2tok = {indx:tok for tok,indx in tok2indx.items()}\n",
    "print('done')\n",
    "print('vocabulary size: {}'.format(len(unigram_counts)))\n",
    "print('most common: {}'.format(unigram_counts.most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0.00% of fenetre\n",
      "done\n",
      "vocabulary size: 5\n",
      "most common: [('r', 3), ('a', 1), ('ê', 1), ('t', 1), ('e', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Maintenant, calculons un vocabulaire unigramme. Le code suivant affecte un ID unique \n",
    "#à chaque jeton, stocke ce mappage dans deux dictionnaires (tok2indx et indx2tok) \n",
    "#et compte la fréquence à laquelle chaque jeton apparaît dans le corpus.\n",
    "\n",
    "tok2indx = dict()\n",
    "unigram_counts = Counter()\n",
    "for ii, fenetre in enumerate(fenetre):\n",
    "    if ii % 200000 == 0:\n",
    "        print(f'finished {ii/len(fenetre):.2%} of fenetre')\n",
    "    for token in fenetre:\n",
    "        unigram_counts[token] += 1\n",
    "        if token not in tok2indx:\n",
    "            tok2indx[token] = len(tok2indx)\n",
    "indx2tok = {indx:tok for tok,indx in tok2indx.items()}\n",
    "print('done')\n",
    "print('vocabulary size: {}'.format(len(unigram_counts)))\n",
    "print('most common: {}'.format(unigram_counts.most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0.00% of fenetre\n",
      "done\n",
      "number of skipgrams: 0\n",
      "most common: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# note add dynammic window hyperparameter\n",
    "back_window = 2\n",
    "front_window = 2\n",
    "skipgram_counts = Counter()\n",
    "for ifenetre, fenetre in enumerate(fenetre):\n",
    "    for ifw, fw in enumerate(fenetre):\n",
    "        icw_min = max(0, ifw - back_window)\n",
    "        icw_max = min(len(fenetre) - 1, ifw + front_window)\n",
    "        icws = [ii for ii in range(icw_min, icw_max + 1) if ii != ifw]\n",
    "        for icw in icws:\n",
    "            skipgram = (fenetre[ifw], fenetre[icw])\n",
    "            skipgram_counts[skipgram] += 1    \n",
    "    if ifenetre % 200000 == 0:\n",
    "        print(f'finished {ifenetre/len(fenetre):.2%} of fenetre')\n",
    "        \n",
    "print('done')\n",
    "print('number of skipgrams: {}'.format(len(skipgram_counts)))\n",
    "print('most common: {}'.format(skipgram_counts.most_common(10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
